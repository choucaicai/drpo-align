
# Optimizing Preference Alignment with Differentiable NDCG Ranking

This repository contains the implementation of the paper "Optimizing Preference Alignment with Differentiable NDCG Ranking".

Full code will be available soon. We are currently in the process of organizing and refining all the code. Some portions of the code have already been uploaded.


## Overview

This project introduces a novel approach to optimize preference alignment using a diffNDCG. Our method allows for end-to-end training of ranking models while directly optimizing for NDCG, leading to improved performance in preference alignment tasks.

## Prerequisites
### Installation 

To set up the project environment:

1. Clone the repository:

```
```
<!-- 
```
git clone https://github.com/yourusername/differentiable-ndcg-ranking.git
cd differentiable-ndcg-ranking
```

2. Create a virtual environment (optional but recommended):

```
python -m venv venv
source venv/bin/activate # On Windows, use venv\Scripts\activate
```

3. Install the required packages:
```
pip install -r requirements.txt
```

## Usage

### Training

To train the model:
```

```
### Evaluation

To evaluate a trained model:
```

```

## Project Structure

```
drpo-align
│
├── data/ # Training and evaluation scripts
├── eval/ # Training and evaluation scripts
├── recipes/ # Training and evaluation scripts
├── scripts/ # Training and evaluation scripts
├── src/
│ ├── data/ # Data loading and preprocessing
│ ├── models/ # Model implementations
│ ├── training/ # Training loops and loss functions
│ └── utils/ # Utility functions and metrics
``` -->
